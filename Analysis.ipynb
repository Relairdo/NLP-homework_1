{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import argparse\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from scipy.sparse.construct import vstack\r\n",
    "from features import get_dataframe, update_text, update_ngrams, update_lexicon, upadate_linguistic, update_user, get_features, get_lable\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, plot_confusion_matrix\r\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('--train', dest='train', required=False, default='data/train.jsonl',\r\n",
    "                    help='Full path to the training file')\r\n",
    "parser.add_argument('--test', dest='test', required=False, default='data/val.jsonl',\r\n",
    "                    help='Full path to the evaluation file')\r\n",
    "parser.add_argument('--user_data', dest='user_data', required=False, default='data/users.json',\r\n",
    "                    help='Full path to the user data file')\r\n",
    "parser.add_argument('--model', dest='model', required=False, default='Ngram+Lex+Ling+User',\r\n",
    "                    choices=[\"Ngram\", \"Ngram+Lex\", \"Ngram+Lex+Ling\", \"Ngram+Lex+Ling+User\"],\r\n",
    "                    help='The name of the model to train and evaluate.')\r\n",
    "parser.add_argument('--lexicon_path', dest='lexicon_path', required=False, default='lexica/',\r\n",
    "                    help='The full path to the directory containing the lexica.'\r\n",
    "                            ' The last folder of this path should be \"lexica\".')\r\n",
    "parser.add_argument('--outfile', dest='outfile', required=False, default='out.txt',\r\n",
    "                    help='Full path to the file we will write the model predictions')\r\n",
    "                    \r\n",
    "args = parser.parse_args(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "folderpath = os.path.join('pickle')\r\n",
    "if os.path.isdir(folderpath):\r\n",
    "    pass\r\n",
    "else:\r\n",
    "    os.mkdir(folderpath)\r\n",
    "\r\n",
    "df_train_loc = os.path.join(folderpath, 'df_train.pkl')\r\n",
    "df_test_loc = os.path.join(folderpath, 'df_test.pkl')\r\n",
    "df_user_loc = os.path.join(folderpath, 'df_user.pkl')\r\n",
    "\r\n",
    "\r\n",
    "if os.path.isfile(df_train_loc) and os.path.isfile(df_test_loc):\r\n",
    "    df_train = pd.read_pickle(df_train_loc)\r\n",
    "    df_test = pd.read_pickle(df_test_loc)\r\n",
    "    df_user = pd.read_pickle(df_user_loc)\r\n",
    "\r\n",
    "else:\r\n",
    "\r\n",
    "    start = time.time()\r\n",
    "    \r\n",
    "    df_train, df_test = get_dataframe(args.train, args.test)\r\n",
    "    update_text(df_train, df_test)\r\n",
    "    update_ngrams(df_train, df_test, feature_number=9800)\r\n",
    "    update_lexicon(df_train, df_test, args.lexicon_path)\r\n",
    "    upadate_linguistic(df_train, df_test)\r\n",
    "    df_train, df_test, df_user = update_user(df_train, df_test, args.user_data)\r\n",
    "\r\n",
    "    end = time.time()\r\n",
    "    print(\"Data Preprocessiong Cost:\", round(end - start),'s.')\r\n",
    "\r\n",
    "    df_train.to_pickle(df_train_loc)\r\n",
    "    df_test.to_pickle(df_test_loc)\r\n",
    "    df_user.to_pickle(df_user_loc)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\fff32\\Desktop\\Natural Language Processing\\Homework_1\\code\\features.py:67: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_CL = pd.read_csv(CL_csv, sep=\"_|,\", names=['word', 'part', 'sentiment'])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Preprocessiong Cost: 93 s.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import itertools\r\n",
    "def get_all_combinations(l : list, choose2=False, up22=False) -> list:\r\n",
    "    ll = []\r\n",
    "    for L in range(0, len(l)+1):\r\n",
    "        for subset in itertools.combinations(l, L):\r\n",
    "            if up22:\r\n",
    "                if len(list(subset)) <= 2:\r\n",
    "                    ll.append(list(subset))\r\n",
    "            elif choose2:\r\n",
    "                if len(list(subset)) == 2:\r\n",
    "                    ll.append(list(subset))\r\n",
    "            else:\r\n",
    "                ll.append(list(subset))\r\n",
    "    return ll\r\n",
    "\r\n",
    "lexicons_list = [\"CL\", \"NVL\"]\r\n",
    "ling_feature_list = ['Length', 'R2O', 'Personal_pronouns', 'Modals', 'Links', 'Questions']\r\n",
    "user_feature_list = ['education','ethnicity', 'gender', 'income', 'joined', 'party', 'political_ideology', 'relationship', 'religious_ideology']\r\n",
    "all_feature_list = list(lexicons_list+ling_feature_list+user_feature_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare L2 vs N/A 5FCV => N/A is better"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "column_names = [\"Model\", \"Norm\",\"5F-CV Mean\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "for md in [\"Ngram\", \"Ngram+Lex\", \"Ngram+Lex+Ling\", \"Ngram+Lex+Ling+User\"]:\r\n",
    "    for nrom in [None, \"l2\"]:\r\n",
    "        x_train, x_test = get_features(df_train, df_test, df_user, model = md, norm=nrom, )\r\n",
    "        y_train, y_test = get_lable(df_train, df_test)\r\n",
    "        x = vstack([x_train,x_test])\r\n",
    "        y = y_train + y_test\r\n",
    "        clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "        scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "        mean_score = np.mean(scores)\r\n",
    "        record = {\"Model\": md, \"Norm\":nrom, \"5F-CV Mean\":mean_score}\r\n",
    "        df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "df_record.to_csv(os.path.join('log/L2.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proformance test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "for md in [\"Ngram+Lex+Ling+User\"]:\r\n",
    "    print('====================', md,'====================')\r\n",
    "    x_train, x_test = get_features(df_train, df_test, df_user, model = md)\r\n",
    "    y_train, y_test = get_lable(df_train, df_test)\r\n",
    "    print(\"Total Featurs:\", x_train.shape[1])\r\n",
    "    clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "    clf.fit(x_train, y_train)\r\n",
    "    y_predicted = clf.predict(x_test)\r\n",
    "    print(\"Accuracy score: \",accuracy_score(y_test, y_predicted))\r\n",
    "    print(\"Accuracy score on Train: \",accuracy_score(y_train, clf.predict(x_train)))\r\n",
    "    # plot_confusion_matrix(clf, x_test, y_test)\r\n",
    "    # print(classification_report(y_test, y_predicted, target_names=['Con','Pro']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================== Ngram+Lex+Ling+User ====================\n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Total Featurs: 20060\n",
      "Accuracy score:  0.7994987468671679\n",
      "Accuracy score on Train:  0.8222361809045227\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "df_train_r = df_train[df_train['category']=='Religion']\r\n",
    "df_train_nr = df_train[df_train['category']!='Religion']\r\n",
    "df_test_r = df_test[df_test['category']=='Religion']\r\n",
    "df_test_nr = df_test[df_test['category']!='Religion']\r\n",
    "\r\n",
    "for md in [\"Ngram+Lex+Ling+User\"]:\r\n",
    "    for religion in [True, False]:\r\n",
    "        \r\n",
    "        if religion:\r\n",
    "            print(\"====================\",\"R\",md, \"====================\")\r\n",
    "            x_train, x_test = get_features(df_train_r, df_test_r,df_user, model = md)\r\n",
    "            y_train, y_test = get_lable(df_train_r, df_test_r)\r\n",
    "        else:\r\n",
    "            print(\"===================\", \"NR\",md, \"====================\")\r\n",
    "            x_train, x_test = get_features(df_train_nr, df_test_nr,df_user, model = md)\r\n",
    "            y_train, y_test = get_lable(df_train_nr, df_test_nr)\r\n",
    "\r\n",
    "\r\n",
    "        x = vstack([x_train,x_test])\r\n",
    "        y = y_train + y_test\r\n",
    "        print(args.model)\r\n",
    "        clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "        start = time.time()\r\n",
    "        scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "        end = time.time()\r\n",
    "        print(\"Training Model Cost:\", round(end - start),'s.')\r\n",
    "        print(scores)\r\n",
    "        print(np.mean(scores))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================== R Ngram+Lex+Ling+User ====================\n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Ngram+Lex+Ling+User\n",
      "Training Model Cost: 6 s.\n",
      "[0.70967742 0.75268817 0.59139785 0.61956522 0.7173913 ]\n",
      "0.6781439925198691\n",
      "=================== NR Ngram+Lex+Ling+User ====================\n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Ngram+Lex+Ling+User\n",
      "Training Model Cost: 21 s.\n",
      "[0.77124183 0.77124183 0.74836601 0.75409836 0.8       ]\n",
      "0.7689896067716704\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\r\n",
    "for md in [\"Ngram+Lex+Ling+User\"]:\r\n",
    "    print('====================', md,'====================')\r\n",
    "    x_train, x_test = get_features(df_train, df_test, df_user, model = md)\r\n",
    "    y_train, y_test = get_lable(df_train, df_test)\r\n",
    "    x = vstack([x_train,x_test])\r\n",
    "    y = y_train + y_test\r\n",
    "    print(args.model)\r\n",
    "    clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "    start = time.time()\r\n",
    "    scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "    end = time.time()\r\n",
    "    print(\"Training Model Cost:\", round(end - start),'s.')\r\n",
    "    print(scores)\r\n",
    "    print(np.mean(scores))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================== Ngram+Lex+Ling+User ====================\n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Ngram+Lex+Ling+User\n",
      "Training Model Cost: 29 s.\n",
      "[0.7443609  0.77386935 0.75125628 0.75879397 0.79899497]\n",
      "0.7654550950239921\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Best feature_number"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "column_names = [\"Max Feature\",\"5F-CV Mean\", \"scores\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "for fn in range(9600, 9700, 50):\r\n",
    "    print(fn)\r\n",
    "    df_train = df_train.drop(columns=[\"Pro_ngram\", \"Con_ngram\"])\r\n",
    "    df_test = df_test.drop(columns=[\"Pro_ngram\", \"Con_ngram\"])\r\n",
    "    update_ngrams(df_train, df_test, feature_number=fn)\r\n",
    "    x_train, x_test = get_features(df_train, df_test, model = \"Ngram\")\r\n",
    "    y_train, y_test = get_lable(df_train, df_test)\r\n",
    "    x = vstack([x_train,x_test])\r\n",
    "    y = y_train + y_test\r\n",
    "    clf = LogisticRegression(solver='liblinear')\r\n",
    "    scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "    mean_score = np.mean(scores)\r\n",
    "    record = {\"Max Feature\":fn, \"5F-CV Mean\":mean_score, \"scores\":scores}\r\n",
    "    df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "df_record.to_csv(os.path.join('log/Ngram_FN_4.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross Valid All combination"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "column_names = [\"Lex\",\"Ling\",\"User\",\"5FCV Mean\", \"Scores\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "for lex in get_all_combinations(lexicons_list):\r\n",
    "    for ling in get_all_combinations(ling_feature_list, choose2=True):\r\n",
    "        for user in get_all_combinations(user_feature_list, choose2=True):\r\n",
    "            x_train, x_test = get_features(df_train, df_test, model = args.model,lex_list=lex, ling_list=ling, user_list=user)\r\n",
    "            y_train, y_test = get_lable(df_train, df_test)\r\n",
    "            x = vstack([x_train,x_test])\r\n",
    "            y = y_train + y_test\r\n",
    "            clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "            scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "            mean_score = np.mean(scores)\r\n",
    "            record = {\"Lex\":lex,\"Ling\":ling,\"User\":user,\"5FCV Mean\":mean_score, \"Scores\":scores}\r\n",
    "            df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "df_record.to_csv(os.path.join('Traversal.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 Find Best Ling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "lexicons_list = [\"CL\", \"NVL\"]\r\n",
    "ling_feature_list = ['Length', 'R2O', 'Personal_pronouns', 'Modals', 'Links', 'Questions']\r\n",
    "user_feature_list = ['education','ethnicity', 'gender', 'income', 'joined', 'party', 'political_ideology', 'relationship', 'religious_ideology']\r\n",
    "all_feature_list = list(lexicons_list+ling_feature_list+user_feature_list)\r\n",
    "\r\n",
    "\r\n",
    "column_names = [\"Lex\",\"Ling\",\"5FCV Mean\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "for lex in get_all_combinations(lexicons_list):\r\n",
    "    for ling in get_all_combinations(ling_feature_list, up22=True):\r\n",
    "        x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex+Ling\",lex_list=lex, ling_list=ling, user_list=[])\r\n",
    "        y_train, y_test = get_lable(df_train, df_test)\r\n",
    "        x = vstack([x_train,x_test])\r\n",
    "        y = y_train + y_test\r\n",
    "        clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "        scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "        mean_score = np.mean(scores)\r\n",
    "        record = {\"Lex\":lex,\"Ling\":ling,\"5FCV Mean\":mean_score}\r\n",
    "        df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "        df_record.to_csv(os.path.join('log/3_1.csv'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexicon used: \n",
      "Linguistic features: \n",
      "Lexicon used: \n",
      "Linguistic features: Length \n",
      "Lexicon used: \n",
      "Linguistic features: R2O \n",
      "Lexicon used: \n",
      "Linguistic features: Personal_pronouns \n",
      "Lexicon used: \n",
      "Linguistic features: Modals \n",
      "Lexicon used: \n",
      "Linguistic features: Links \n",
      "Lexicon used: \n",
      "Linguistic features: Questions \n",
      "Lexicon used: \n",
      "Linguistic features: Length R2O \n",
      "Lexicon used: \n",
      "Linguistic features: Length Personal_pronouns \n",
      "Lexicon used: \n",
      "Linguistic features: Length Modals \n",
      "Lexicon used: \n",
      "Linguistic features: Length Links \n",
      "Lexicon used: \n",
      "Linguistic features: Length Questions \n",
      "Lexicon used: \n",
      "Linguistic features: R2O Personal_pronouns \n",
      "Lexicon used: \n",
      "Linguistic features: R2O Modals \n",
      "Lexicon used: \n",
      "Linguistic features: R2O Links \n",
      "Lexicon used: \n",
      "Linguistic features: R2O Questions \n",
      "Lexicon used: \n",
      "Linguistic features: Personal_pronouns Modals \n",
      "Lexicon used: \n",
      "Linguistic features: Personal_pronouns Links \n",
      "Lexicon used: \n",
      "Linguistic features: Personal_pronouns Questions \n",
      "Lexicon used: \n",
      "Linguistic features: Modals Links \n",
      "Lexicon used: \n",
      "Linguistic features: Modals Questions \n",
      "Lexicon used: \n",
      "Linguistic features: Links Questions \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: R2O \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Personal_pronouns \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Modals \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Links \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Questions \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length R2O \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length Personal_pronouns \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length Modals \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length Links \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length Questions \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: R2O Personal_pronouns \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: R2O Modals \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: R2O Links \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: R2O Questions \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Personal_pronouns Modals \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Personal_pronouns Links \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Personal_pronouns Questions \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Modals Links \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Modals Questions \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Links Questions \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: R2O \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Personal_pronouns \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Modals \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Links \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Questions \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length R2O \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length Personal_pronouns \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length Modals \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length Links \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length Questions \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: R2O Personal_pronouns \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: R2O Modals \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: R2O Links \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: R2O Questions \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Personal_pronouns Modals \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Personal_pronouns Links \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Personal_pronouns Questions \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Modals Links \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Modals Questions \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Links Questions \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Personal_pronouns \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Modals \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Links \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Questions \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length Personal_pronouns \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length Modals \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length Links \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length Questions \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O Personal_pronouns \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O Modals \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O Links \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O Questions \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Personal_pronouns Modals \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Personal_pronouns Links \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Personal_pronouns Questions \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Modals Links \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Modals Questions \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Links Questions \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 Examples ['Length', 'Links']"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex+Ling\",lex_list=[\"NVL\"], ling_list=['Length', 'Links'], user_list=[])\r\n",
    "y_train, y_test = get_lable(df_train, df_test)\r\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "P_with_ling = clf.predict(x_test)\r\n",
    "x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex\",lex_list=[\"NVL\"], ling_list=[], user_list=[])\r\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "P = clf.predict(x_test)\r\n",
    "\r\n",
    "winner_length = 0\r\n",
    "loser_length = 0\r\n",
    "winner_links = 0\r\n",
    "loser_links = 0\r\n",
    "for i in range(len(y_test)):\r\n",
    "    if P_with_ling[i] == y_test[i] and P_with_ling[i] != P[i]:\r\n",
    "        print(\"Debate:\", i)\r\n",
    "        report = df_test.loc[i,[\"winner\", \"Pro_Links\",\"Con_Links\",\"Pro_Length\", \"Con_Length\"]].to_dict()\r\n",
    "        print(report)\r\n",
    "        if y_test[i]:\r\n",
    "            winner_length += report[\"Pro_Length\"]\r\n",
    "            winner_links += report[\"Pro_Links\"]\r\n",
    "            loser_length += report[\"Con_Length\"]\r\n",
    "            loser_links += report[\"Con_Links\"]\r\n",
    "\r\n",
    "        else:\r\n",
    "            winner_length += report[\"Con_Length\"]\r\n",
    "            winner_links += report[\"Con_Links\"]\r\n",
    "            loser_length += report[\"Pro_Length\"]\r\n",
    "            loser_links += report[\"Pro_Links\"]\r\n",
    "\r\n",
    "print(\"winner_length:\", winner_length)\r\n",
    "print(\"loser_length\", loser_length)\r\n",
    "print(\"winner_links\", winner_links)\r\n",
    "print(\"loser_links\", loser_links)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length Links \n",
      "Lexicon used: NRC-VAD \n",
      "Debate: 76\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 4, 'Pro_Length': 9300, 'Con_Length': 7957}\n",
      "Debate: 104\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 2035, 'Con_Length': 1448}\n",
      "Debate: 116\n",
      "{'winner': 'Pro', 'Pro_Links': 4, 'Con_Links': 0, 'Pro_Length': 7406, 'Con_Length': 6700}\n",
      "Debate: 132\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 1694, 'Con_Length': 2302}\n",
      "Debate: 149\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 1455, 'Con_Length': 1622}\n",
      "Debate: 166\n",
      "{'winner': 'Pro', 'Pro_Links': 1, 'Con_Links': 0, 'Pro_Length': 7117, 'Con_Length': 5559}\n",
      "Debate: 171\n",
      "{'winner': 'Con', 'Pro_Links': 4, 'Con_Links': 9, 'Pro_Length': 14002, 'Con_Length': 20314}\n",
      "Debate: 181\n",
      "{'winner': 'Con', 'Pro_Links': 6, 'Con_Links': 5, 'Pro_Length': 9353, 'Con_Length': 10972}\n",
      "Debate: 189\n",
      "{'winner': 'Con', 'Pro_Links': 6, 'Con_Links': 2, 'Pro_Length': 3764, 'Con_Length': 3281}\n",
      "Debate: 203\n",
      "{'winner': 'Con', 'Pro_Links': 1, 'Con_Links': 5, 'Pro_Length': 11449, 'Con_Length': 14817}\n",
      "Debate: 206\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 9014, 'Con_Length': 10599}\n",
      "Debate: 240\n",
      "{'winner': 'Pro', 'Pro_Links': 1, 'Con_Links': 0, 'Pro_Length': 4761, 'Con_Length': 1895}\n",
      "Debate: 265\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 743, 'Con_Length': 107}\n",
      "Debate: 306\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 6337, 'Con_Length': 5653}\n",
      "Debate: 309\n",
      "{'winner': 'Con', 'Pro_Links': 0, 'Con_Links': 0, 'Pro_Length': 2182, 'Con_Length': 2686}\n",
      "Debate: 322\n",
      "{'winner': 'Pro', 'Pro_Links': 0, 'Con_Links': 4, 'Pro_Length': 9515, 'Con_Length': 2990}\n",
      "Debate: 324\n",
      "{'winner': 'Pro', 'Pro_Links': 20, 'Con_Links': 15, 'Pro_Length': 29500, 'Con_Length': 29149}\n",
      "Debate: 344\n",
      "{'winner': 'Pro', 'Pro_Links': 3, 'Con_Links': 5, 'Pro_Length': 17648, 'Con_Length': 14254}\n",
      "Debate: 351\n",
      "{'winner': 'Con', 'Pro_Links': 4, 'Con_Links': 9, 'Pro_Length': 8807, 'Con_Length': 5663}\n",
      "Debate: 367\n",
      "{'winner': 'Pro', 'Pro_Links': 2, 'Con_Links': 0, 'Pro_Length': 3393, 'Con_Length': 2083}\n",
      "Debate: 373\n",
      "{'winner': 'Con', 'Pro_Links': 4, 'Con_Links': 6, 'Pro_Length': 9174, 'Con_Length': 8721}\n",
      "winner_length: 175482\n",
      "loser_length 151939\n",
      "winner_links 71\n",
      "loser_links 49\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4_2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex\",lex_list=[\"NVL\"], ling_list=[], user_list=[])\r\n",
    "y_train, y_test = get_lable(df_train, df_test)\r\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "P_with_ling = clf.predict(x_test)\r\n",
    "x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex\",lex_list=[\"CL\"], ling_list=[], user_list=[])\r\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "P = clf.predict(x_test)\r\n",
    "\r\n",
    "winner_positive = []\r\n",
    "winner_neutral = []\r\n",
    "\r\n",
    "for i in range(len(y_test)):\r\n",
    "    if P_with_ling[i] == y_test[i] and P_with_ling[i] != P[i]:\r\n",
    "        print(\"Debate:\", i)\r\n",
    "        report = df_test.loc[i,[\"winner\", 'Pro_positive', 'Con_positive', 'Pro_neutral',\r\n",
    "       'Con_neutral', 'Pro_negative', 'Con_negative', 'Pro_a-score',\r\n",
    "       'Pro_d-score', 'Pro_v-score', 'Con_a-score', 'Con_d-score', 'Con_v-score']]\r\n",
    "        print(report)\r\n",
    "        if y_test[i]:\r\n",
    "\r\n",
    "\r\n",
    "        else:\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# print(\"winner_length:\", winner_length)\r\n",
    "# print(\"loser_length\", loser_length)\r\n",
    "# print(\"winner_links\", winner_links)\r\n",
    "# print(\"loser_links\", loser_links)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexicon used: NRC-VAD \n",
      "Lexicon used: Connotation \n",
      "Debate: 10\n",
      "winner              Pro\n",
      "Pro_positive        253\n",
      "Con_positive        274\n",
      "Pro_neutral         321\n",
      "Con_neutral         358\n",
      "Pro_negative        221\n",
      "Con_negative        261\n",
      "Pro_a-score     964.553\n",
      "Pro_d-score     874.713\n",
      "Pro_v-score     1036.13\n",
      "Con_a-score     940.858\n",
      "Con_d-score     774.323\n",
      "Con_v-score     967.065\n",
      "Name: 10, dtype: object\n",
      "Debate: 14\n",
      "winner              Con\n",
      "Pro_positive        231\n",
      "Con_positive        202\n",
      "Pro_neutral         299\n",
      "Con_neutral         271\n",
      "Pro_negative        225\n",
      "Con_negative        178\n",
      "Pro_a-score     406.614\n",
      "Pro_d-score     353.004\n",
      "Pro_v-score     423.571\n",
      "Con_a-score     368.068\n",
      "Con_d-score     324.231\n",
      "Con_v-score     403.071\n",
      "Name: 14, dtype: object\n",
      "Debate: 20\n",
      "winner              Pro\n",
      "Pro_positive        252\n",
      "Con_positive        256\n",
      "Pro_neutral         308\n",
      "Con_neutral         313\n",
      "Pro_negative        230\n",
      "Con_negative        229\n",
      "Pro_a-score     573.923\n",
      "Pro_d-score     446.834\n",
      "Pro_v-score     555.914\n",
      "Con_a-score     466.504\n",
      "Con_d-score     357.602\n",
      "Con_v-score     450.183\n",
      "Name: 20, dtype: object\n",
      "Debate: 42\n",
      "winner              Pro\n",
      "Pro_positive        169\n",
      "Con_positive        192\n",
      "Pro_neutral         239\n",
      "Con_neutral         253\n",
      "Pro_negative        137\n",
      "Con_negative        182\n",
      "Pro_a-score     786.745\n",
      "Pro_d-score     575.614\n",
      "Pro_v-score     779.795\n",
      "Con_a-score     648.758\n",
      "Con_d-score     494.869\n",
      "Con_v-score     660.971\n",
      "Name: 42, dtype: object\n",
      "Debate: 55\n",
      "winner              Pro\n",
      "Pro_positive        153\n",
      "Con_positive        213\n",
      "Pro_neutral         190\n",
      "Con_neutral         280\n",
      "Pro_negative        138\n",
      "Con_negative        187\n",
      "Pro_a-score     710.116\n",
      "Pro_d-score     600.924\n",
      "Pro_v-score     735.632\n",
      "Con_a-score     660.227\n",
      "Con_d-score     536.997\n",
      "Con_v-score     661.395\n",
      "Name: 55, dtype: object\n",
      "Debate: 69\n",
      "winner              Con\n",
      "Pro_positive         46\n",
      "Con_positive         64\n",
      "Pro_neutral          47\n",
      "Con_neutral          65\n",
      "Pro_negative         24\n",
      "Con_negative         44\n",
      "Pro_a-score      73.946\n",
      "Pro_d-score      47.922\n",
      "Pro_v-score      64.957\n",
      "Con_a-score     107.033\n",
      "Con_d-score      71.856\n",
      "Con_v-score      95.978\n",
      "Name: 69, dtype: object\n",
      "Debate: 74\n",
      "winner             Pro\n",
      "Pro_positive        33\n",
      "Con_positive        32\n",
      "Pro_neutral         44\n",
      "Con_neutral         33\n",
      "Pro_negative        29\n",
      "Con_negative        13\n",
      "Pro_a-score     50.413\n",
      "Pro_d-score     40.258\n",
      "Pro_v-score     51.684\n",
      "Con_a-score      38.44\n",
      "Con_d-score     28.219\n",
      "Con_v-score     37.093\n",
      "Name: 74, dtype: object\n",
      "Debate: 81\n",
      "winner              Pro\n",
      "Pro_positive        106\n",
      "Con_positive        173\n",
      "Pro_neutral         133\n",
      "Con_neutral         207\n",
      "Pro_negative         76\n",
      "Con_negative        101\n",
      "Pro_a-score     198.038\n",
      "Pro_d-score     144.109\n",
      "Pro_v-score       191.7\n",
      "Con_a-score     317.478\n",
      "Con_d-score     221.063\n",
      "Con_v-score     305.152\n",
      "Name: 81, dtype: object\n",
      "Debate: 89\n",
      "winner              Pro\n",
      "Pro_positive         92\n",
      "Con_positive         72\n",
      "Pro_neutral          88\n",
      "Con_neutral          78\n",
      "Pro_negative         76\n",
      "Con_negative         70\n",
      "Pro_a-score     176.708\n",
      "Pro_d-score     162.901\n",
      "Pro_v-score      200.32\n",
      "Con_a-score     118.524\n",
      "Con_d-score     120.814\n",
      "Con_v-score     139.806\n",
      "Name: 89, dtype: object\n",
      "Debate: 91\n",
      "winner              Con\n",
      "Pro_positive        294\n",
      "Con_positive        217\n",
      "Pro_neutral         294\n",
      "Con_neutral         234\n",
      "Pro_negative        159\n",
      "Con_negative        161\n",
      "Pro_a-score     588.008\n",
      "Pro_d-score     427.447\n",
      "Pro_v-score     579.099\n",
      "Con_a-score     431.405\n",
      "Con_d-score     350.394\n",
      "Con_v-score     433.249\n",
      "Name: 91, dtype: object\n",
      "Debate: 110\n",
      "winner             Con\n",
      "Pro_positive        10\n",
      "Con_positive         8\n",
      "Pro_neutral         20\n",
      "Con_neutral          8\n",
      "Pro_negative        12\n",
      "Con_negative         8\n",
      "Pro_a-score     18.249\n",
      "Pro_d-score     15.716\n",
      "Pro_v-score     18.478\n",
      "Con_a-score      8.543\n",
      "Con_d-score      6.848\n",
      "Con_v-score      8.447\n",
      "Name: 110, dtype: object\n",
      "Debate: 118\n",
      "winner              Con\n",
      "Pro_positive        263\n",
      "Con_positive        299\n",
      "Pro_neutral         325\n",
      "Con_neutral         350\n",
      "Pro_negative        240\n",
      "Con_negative        243\n",
      "Pro_a-score     621.475\n",
      "Pro_d-score     550.243\n",
      "Pro_v-score      651.56\n",
      "Con_a-score     721.443\n",
      "Con_d-score     620.982\n",
      "Con_v-score     749.145\n",
      "Name: 118, dtype: object\n",
      "Debate: 153\n",
      "winner              Pro\n",
      "Pro_positive         33\n",
      "Con_positive         64\n",
      "Pro_neutral          37\n",
      "Con_neutral          80\n",
      "Pro_negative         20\n",
      "Con_negative         44\n",
      "Pro_a-score     103.066\n",
      "Pro_d-score      80.535\n",
      "Pro_v-score      95.301\n",
      "Con_a-score      79.331\n",
      "Con_d-score      66.216\n",
      "Con_v-score      68.158\n",
      "Name: 153, dtype: object\n",
      "Debate: 185\n",
      "winner              Con\n",
      "Pro_positive        218\n",
      "Con_positive        217\n",
      "Pro_neutral         291\n",
      "Con_neutral         294\n",
      "Pro_negative        198\n",
      "Con_negative        184\n",
      "Pro_a-score     520.529\n",
      "Pro_d-score     428.953\n",
      "Pro_v-score     502.926\n",
      "Con_a-score      575.09\n",
      "Con_d-score     494.113\n",
      "Con_v-score     570.803\n",
      "Name: 185, dtype: object\n",
      "Debate: 188\n",
      "winner             Con\n",
      "Pro_positive        62\n",
      "Con_positive        26\n",
      "Pro_neutral         70\n",
      "Con_neutral         37\n",
      "Pro_negative        55\n",
      "Con_negative        30\n",
      "Pro_a-score     76.473\n",
      "Pro_d-score     70.485\n",
      "Pro_v-score     76.014\n",
      "Con_a-score     36.923\n",
      "Con_d-score     39.842\n",
      "Con_v-score     39.929\n",
      "Name: 188, dtype: object\n",
      "Debate: 211\n",
      "winner              Pro\n",
      "Pro_positive        200\n",
      "Con_positive        151\n",
      "Pro_neutral         235\n",
      "Con_neutral         210\n",
      "Pro_negative        200\n",
      "Con_negative        152\n",
      "Pro_a-score     540.232\n",
      "Pro_d-score     475.387\n",
      "Pro_v-score     539.297\n",
      "Con_a-score     303.419\n",
      "Con_d-score     260.885\n",
      "Con_v-score     303.274\n",
      "Name: 211, dtype: object\n",
      "Debate: 217\n",
      "winner              Con\n",
      "Pro_positive         98\n",
      "Con_positive        119\n",
      "Pro_neutral         161\n",
      "Con_neutral         163\n",
      "Pro_negative        108\n",
      "Con_negative        103\n",
      "Pro_a-score     219.741\n",
      "Pro_d-score       201.2\n",
      "Pro_v-score     246.329\n",
      "Con_a-score      275.33\n",
      "Con_d-score     241.624\n",
      "Con_v-score     293.417\n",
      "Name: 217, dtype: object\n",
      "Debate: 221\n",
      "winner              Pro\n",
      "Pro_positive        192\n",
      "Con_positive        192\n",
      "Pro_neutral         232\n",
      "Con_neutral         242\n",
      "Pro_negative        163\n",
      "Con_negative        145\n",
      "Pro_a-score     505.109\n",
      "Pro_d-score     398.415\n",
      "Pro_v-score      501.31\n",
      "Con_a-score     448.811\n",
      "Con_d-score      335.28\n",
      "Con_v-score     448.112\n",
      "Name: 221, dtype: object\n",
      "Debate: 337\n",
      "winner              Con\n",
      "Pro_positive        150\n",
      "Con_positive        124\n",
      "Pro_neutral         192\n",
      "Con_neutral         149\n",
      "Pro_negative        105\n",
      "Con_negative         89\n",
      "Pro_a-score     330.966\n",
      "Pro_d-score     230.594\n",
      "Pro_v-score     319.266\n",
      "Con_a-score     367.346\n",
      "Con_d-score     263.554\n",
      "Con_v-score      356.14\n",
      "Name: 337, dtype: object\n",
      "Debate: 359\n",
      "winner              Pro\n",
      "Pro_positive        103\n",
      "Con_positive         39\n",
      "Pro_neutral         115\n",
      "Con_neutral          46\n",
      "Pro_negative         78\n",
      "Con_negative         30\n",
      "Pro_a-score     227.219\n",
      "Pro_d-score     201.244\n",
      "Pro_v-score     228.964\n",
      "Con_a-score      79.608\n",
      "Con_d-score      71.097\n",
      "Con_v-score      85.304\n",
      "Name: 359, dtype: object\n",
      "Debate: 363\n",
      "winner              Pro\n",
      "Pro_positive        120\n",
      "Con_positive        157\n",
      "Pro_neutral         146\n",
      "Con_neutral         204\n",
      "Pro_negative         82\n",
      "Con_negative        114\n",
      "Pro_a-score     198.231\n",
      "Pro_d-score     154.595\n",
      "Pro_v-score     205.989\n",
      "Con_a-score     266.098\n",
      "Con_d-score     211.073\n",
      "Con_v-score     267.326\n",
      "Name: 363, dtype: object\n",
      "Debate: 368\n",
      "winner              Pro\n",
      "Pro_positive        147\n",
      "Con_positive        169\n",
      "Pro_neutral         209\n",
      "Con_neutral         229\n",
      "Pro_negative        150\n",
      "Con_negative        153\n",
      "Pro_a-score     441.528\n",
      "Pro_d-score     393.314\n",
      "Pro_v-score     436.344\n",
      "Con_a-score     398.164\n",
      "Con_d-score     369.837\n",
      "Con_v-score     397.024\n",
      "Name: 368, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex+Ling+User\",lex_list=[\"NVL\"], ling_list=['Links', 'Questions'], user_list=['education', 'party'])\r\n",
    "y_train, y_test = get_lable(df_train, df_test)\r\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "P_with_ling = clf.predict(x_test)\r\n",
    "x_train, x_test = get_features(df_train, df_test, df_user, model = \"Ngram+Lex+Ling\",lex_list=[\"NVL\"], ling_list=['Links', 'Questions'], user_list=[])\r\n",
    "clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "P = clf.predict(x_test)\r\n",
    "\r\n",
    "print(\"Accuracy score\",accuracy_score(y_test, P))\r\n",
    "print(\"Accuracy score with user\",accuracy_score(y_test, P_with_ling))\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(y_test)):\r\n",
    "    if P_with_ling[i] == y_test[i] and P_with_ling[i] != P[i]:\r\n",
    "        print(\"Debate:\", i)\r\n",
    "        report = df_test.loc[i,[\"winner\", 'Pro_education', 'Con_education', 'Pro_party','Con_party']]\r\n",
    "        print(report)\r\n",
    "        # if y_test[i]:\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Links Questions \n",
      "User features: education party \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Links Questions \n",
      "Accuracy score 0.7593984962406015\n",
      "Accuracy score with user 0.7919799498746867\n",
      "Debate: 4\n",
      "winner                       Pro\n",
      "Pro_education    Graduate Degree\n",
      "Con_education    Graduate Degree\n",
      "Pro_party              Undecided\n",
      "Con_party            Independent\n",
      "Name: 4, dtype: object\n",
      "Debate: 15\n",
      "winner                    Pro\n",
      "Pro_education    Some College\n",
      "Con_education      Not Saying\n",
      "Pro_party               Other\n",
      "Con_party          Not Saying\n",
      "Name: 15, dtype: object\n",
      "Debate: 76\n",
      "winner                          Con\n",
      "Pro_education            Not Saying\n",
      "Con_education           High School\n",
      "Pro_party                Not Saying\n",
      "Con_party        Independence Party\n",
      "Name: 76, dtype: object\n",
      "Debate: 79\n",
      "winner                       Pro\n",
      "Pro_education    Graduate Degree\n",
      "Con_education         Not Saying\n",
      "Pro_party            Independent\n",
      "Con_party             Not Saying\n",
      "Name: 79, dtype: object\n",
      "Debate: 94\n",
      "winner                        Con\n",
      "Pro_education         High School\n",
      "Con_education    Bachelors Degree\n",
      "Pro_party        Republican Party\n",
      "Con_party               Undecided\n",
      "Name: 94, dtype: object\n",
      "Debate: 103\n",
      "winner                        Pro\n",
      "Pro_education     Graduate Degree\n",
      "Con_education          Not Saying\n",
      "Pro_party        Democratic Party\n",
      "Con_party        Democratic Party\n",
      "Name: 103, dtype: object\n",
      "Debate: 105\n",
      "winner                         Pro\n",
      "Pro_education     Bachelors Degree\n",
      "Con_education           Not Saying\n",
      "Pro_party        Libertarian Party\n",
      "Con_party         Republican Party\n",
      "Name: 105, dtype: object\n",
      "Debate: 109\n",
      "winner                    Con\n",
      "Pro_education      Not Saying\n",
      "Con_education    Some College\n",
      "Pro_party          Not Saying\n",
      "Con_party               Other\n",
      "Name: 109, dtype: object\n",
      "Debate: 118\n",
      "winner                    Con\n",
      "Pro_education    Some College\n",
      "Con_education    Some College\n",
      "Pro_party         Independent\n",
      "Con_party           Undecided\n",
      "Name: 118, dtype: object\n",
      "Debate: 132\n",
      "winner                        Con\n",
      "Pro_education          Not Saying\n",
      "Con_education    Bachelors Degree\n",
      "Pro_party              Not Saying\n",
      "Con_party        Republican Party\n",
      "Name: 132, dtype: object\n",
      "Debate: 164\n",
      "winner                           Pro\n",
      "Pro_education            High School\n",
      "Con_education                  Other\n",
      "Pro_party                  Undecided\n",
      "Con_party        We The People Party\n",
      "Name: 164, dtype: object\n",
      "Debate: 166\n",
      "winner                        Pro\n",
      "Pro_education    Bachelors Degree\n",
      "Con_education         High School\n",
      "Pro_party        Democratic Party\n",
      "Con_party              Not Saying\n",
      "Name: 166, dtype: object\n",
      "Debate: 171\n",
      "winner                    Con\n",
      "Pro_education      Not Saying\n",
      "Con_education    Some College\n",
      "Pro_party          Not Saying\n",
      "Con_party           Undecided\n",
      "Name: 171, dtype: object\n",
      "Debate: 181\n",
      "winner                        Con\n",
      "Pro_education          Not Saying\n",
      "Con_education        Some College\n",
      "Pro_party              Not Saying\n",
      "Con_party        Republican Party\n",
      "Name: 181, dtype: object\n",
      "Debate: 184\n",
      "winner                        Pro\n",
      "Pro_education               Other\n",
      "Con_education          Not Saying\n",
      "Pro_party               Undecided\n",
      "Con_party        Republican Party\n",
      "Name: 184, dtype: object\n",
      "Debate: 189\n",
      "winner                        Con\n",
      "Pro_education         High School\n",
      "Con_education    Bachelors Degree\n",
      "Pro_party        Republican Party\n",
      "Con_party                   Other\n",
      "Name: 189, dtype: object\n",
      "Debate: 212\n",
      "winner                        Con\n",
      "Pro_education          Not Saying\n",
      "Con_education     Graduate Degree\n",
      "Pro_party        Republican Party\n",
      "Con_party        Republican Party\n",
      "Name: 212, dtype: object\n",
      "Debate: 236\n",
      "winner                         Pro\n",
      "Pro_education    Associates Degree\n",
      "Con_education          High School\n",
      "Pro_party                Undecided\n",
      "Con_party         Republican Party\n",
      "Name: 236, dtype: object\n",
      "Debate: 243\n",
      "winner                        Pro\n",
      "Pro_education        Some College\n",
      "Con_education          Not Saying\n",
      "Pro_party        Democratic Party\n",
      "Con_party              Not Saying\n",
      "Name: 243, dtype: object\n",
      "Debate: 275\n",
      "winner                        Con\n",
      "Pro_education         High School\n",
      "Con_education    Bachelors Degree\n",
      "Pro_party        Republican Party\n",
      "Con_party               Undecided\n",
      "Name: 275, dtype: object\n",
      "Debate: 285\n",
      "winner                        Pro\n",
      "Pro_education    Bachelors Degree\n",
      "Con_education         High School\n",
      "Pro_party                   Other\n",
      "Con_party               Undecided\n",
      "Name: 285, dtype: object\n",
      "Debate: 324\n",
      "winner                        Pro\n",
      "Pro_education    Bachelors Degree\n",
      "Con_education         High School\n",
      "Pro_party             Independent\n",
      "Con_party         Communist Party\n",
      "Name: 324, dtype: object\n",
      "Debate: 335\n",
      "winner                        Pro\n",
      "Pro_education    Bachelors Degree\n",
      "Con_education         High School\n",
      "Pro_party        Democratic Party\n",
      "Con_party              Not Saying\n",
      "Name: 335, dtype: object\n",
      "Debate: 344\n",
      "winner                        Pro\n",
      "Pro_education        Some College\n",
      "Con_education    Bachelors Degree\n",
      "Pro_party               Undecided\n",
      "Con_party              Not Saying\n",
      "Name: 344, dtype: object\n",
      "Debate: 351\n",
      "winner                        Con\n",
      "Pro_education         High School\n",
      "Con_education    Bachelors Degree\n",
      "Pro_party               Undecided\n",
      "Con_party               Undecided\n",
      "Name: 351, dtype: object\n",
      "Debate: 360\n",
      "winner                       Pro\n",
      "Pro_education    Graduate Degree\n",
      "Con_education              Other\n",
      "Pro_party             Not Saying\n",
      "Con_party              Undecided\n",
      "Name: 360, dtype: object\n",
      "Debate: 367\n",
      "winner                        Pro\n",
      "Pro_education        Some College\n",
      "Con_education         High School\n",
      "Pro_party        Republican Party\n",
      "Con_party               Undecided\n",
      "Name: 367, dtype: object\n",
      "Debate: 395\n",
      "winner                       Con\n",
      "Pro_education        High School\n",
      "Con_education    Graduate Degree\n",
      "Pro_party             Not Saying\n",
      "Con_party             Not Saying\n",
      "Name: 395, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "df_test[[\"winner\", \"Pro_Links\",\"Con_Links\",\"Pro_Length\", \"Con_Length\"]].iloc[76, :].values[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Con'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "column_names = [\"Ablation Feature\",\"5FCV mean\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "\r\n",
    "for feature in all_feature_list:\r\n",
    "\r\n",
    "    lexicons_list = [\"CL\", \"NVL\"]\r\n",
    "    ling_feature_list = ['Length', 'R2O', 'Personal_pronouns', 'Modals', 'Links', 'Questions']\r\n",
    "    user_feature_list = ['education', 'ethnicity','gender','income','joined','party','politi|cal_ideology','relationship','religious_ideology']\r\n",
    "        \r\n",
    "    if feature in ling_feature_list:\r\n",
    "        ling_feature_list.remove(feature)\r\n",
    "    if feature in user_feature_list:\r\n",
    "        user_feature_list.remove(feature)\r\n",
    "    if feature in lexicons_list:\r\n",
    "        lexicons_list.remove(feature)\r\n",
    "\r\n",
    "    x_train, x_test = get_features(df_train, df_test, df_user, model = args.model,lex_list=lexicons_list, ling_list=ling_feature_list, user_list=user_feature_list)\r\n",
    "    y_train, y_test = get_lable(df_train, df_test)\r\n",
    "\r\n",
    "    x = vstack([x_train,x_test])\r\n",
    "    y = y_train + y_test\r\n",
    "    clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "    scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "    mean_score = np.mean(scores)\r\n",
    "    record = {\"Ablation Feature\":feature ,\"5FCV mean\": mean_score}\r\n",
    "    df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "\r\n",
    "df_record.to_csv(os.path.join('log/Ablation.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation R vs O"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "df_train_r = df_train[df_train['category']=='Religion']\r\n",
    "df_train_nr = df_train[df_train['category']!='Religion']\r\n",
    "df_test_r = df_test[df_test['category']=='Religion']\r\n",
    "df_test_nr = df_test[df_test['category']!='Religion']\r\n",
    "\r\n",
    "column_names = [\"Religion\",\"Ablation Feature\",\"5FCV Mean\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "\r\n",
    "\r\n",
    "for feature in all_feature_list:\r\n",
    "\r\n",
    "    ling_feature_list = ['Length', 'R2O', 'Personal_pronouns', 'Modals', 'Links', 'Questions']\r\n",
    "    user_feature_list = ['education','ethnicity', 'gender', 'income', 'joined', 'party', 'political_ideology', 'relationship', 'religious_ideology']\r\n",
    "    lexicons_list = [\"CL\", \"NVL\"]\r\n",
    "        \r\n",
    "    if feature in ling_feature_list:\r\n",
    "        ling_feature_list.remove(feature)\r\n",
    "    if feature in user_feature_list:\r\n",
    "        user_feature_list.remove(feature)\r\n",
    "    if feature in lexicons_list:\r\n",
    "        lexicons_list.remove(feature)\r\n",
    "\r\n",
    "    for religion in [True, False]:\r\n",
    "\r\n",
    "        if religion:\r\n",
    "            x_train, x_test = get_features(df_train_r, df_test_r, df_user, norm=None, model = args.model,lex_list=lexicons_list, ling_list=ling_feature_list, user_list=user_feature_list)\r\n",
    "            y_train, y_test = get_lable(df_train_r, df_test_r)\r\n",
    "        else:\r\n",
    "            x_train, x_test = get_features(df_train_nr, df_test_nr,df_user, norm=None, model = args.model,lex_list=lexicons_list, ling_list=ling_feature_list, user_list=user_feature_list)\r\n",
    "            y_train, y_test = get_lable(df_train_nr, df_test_nr)\r\n",
    "\r\n",
    "        x = vstack([x_train,x_test])\r\n",
    "        y = y_train + y_test\r\n",
    "        clf = LogisticRegression(solver='liblinear', max_iter=500)\r\n",
    "        start = time.time()\r\n",
    "        scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "        end = time.time()\r\n",
    "        mean_score = np.mean(scores)\r\n",
    "        record = {\"Religion\":religion, \"Ablation Feature\":feature ,\"5FCV Mean\":mean_score}\r\n",
    "        df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "\r\n",
    "df_record.to_csv(os.path.join('log/Ablation_ReligionVsOther.csv'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links \n",
      "User features: education ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: ethnicity gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education gender income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity income joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender joined party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income party political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined political_ideology relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party relationship religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology religious_ideology \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship \n",
      "Lexicon used: Connotation NRC-VAD \n",
      "Linguistic features: Length R2O Personal_pronouns Modals Links Questions \n",
      "User features: education ethnicity gender income joined party political_ideology relationship \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('NLP-hw1': conda)"
  },
  "interpreter": {
   "hash": "71e2e5b76bcbb14e570851f735b138803c85a1bc8fc18edcb67e91ecf28dd39d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}