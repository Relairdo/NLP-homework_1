{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import argparse\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "import os\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from scipy.sparse.construct import vstack\r\n",
    "from features import get_dataframe, update_text, update_ngrams, update_lexicon, upadate_linguistic, update_user, get_features, get_lable\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, plot_confusion_matrix\r\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('--train', dest='train', required=False, default='data/train.jsonl',\r\n",
    "                    help='Full path to the training file')\r\n",
    "parser.add_argument('--test', dest='test', required=False, default='data/val.jsonl',\r\n",
    "                    help='Full path to the evaluation file')\r\n",
    "parser.add_argument('--user_data', dest='user_data', required=False, default='data/users.json',\r\n",
    "                    help='Full path to the user data file')\r\n",
    "parser.add_argument('--model', dest='model', required=False, default='Ngram+Lex+Ling+User',\r\n",
    "                    choices=[\"Ngram\", \"Ngram+Lex\", \"Ngram+Lex+Ling\", \"Ngram+Lex+Ling+User\"],\r\n",
    "                    help='The name of the model to train and evaluate.')\r\n",
    "parser.add_argument('--lexicon_path', dest='lexicon_path', required=False, default='lexica/',\r\n",
    "                    help='The full path to the directory containing the lexica.'\r\n",
    "                            ' The last folder of this path should be \"lexica\".')\r\n",
    "parser.add_argument('--outfile', dest='outfile', required=False, default='out.txt',\r\n",
    "                    help='Full path to the file we will write the model predictions')\r\n",
    "                    \r\n",
    "args = parser.parse_args(\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train_loc = os.path.join('df_train.pkl')\r\n",
    "df_test_loc = os.path.join('df_test.pkl')\r\n",
    "\r\n",
    "\r\n",
    "if os.path.isfile(df_train_loc) and os.path.isfile(df_test_loc):\r\n",
    "    df_train = pd.read_pickle(df_train_loc)\r\n",
    "    df_test = pd.read_pickle(df_test_loc)\r\n",
    "\r\n",
    "else:\r\n",
    "\r\n",
    "    start = time.time()\r\n",
    "\r\n",
    "    df_train, df_test = get_dataframe(args.train, args.test)\r\n",
    "    update_text(df_train, df_test)\r\n",
    "    update_ngrams(df_train, df_test)\r\n",
    "    update_lexicon(df_train, df_test, args.lexicon_path)\r\n",
    "    upadate_linguistic(df_train, df_test)\r\n",
    "    update_user(df_train, df_test, args.user_data)\r\n",
    "\r\n",
    "    end = time.time()\r\n",
    "    print(\"Data Preprocessiong Cost:\", round(end - start),'s.')\r\n",
    "\r\n",
    "    df_train.to_pickle(df_train_loc)\r\n",
    "    df_test.to_pickle(df_test_loc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train_r = df_train[df_train['category']=='Religion']\r\n",
    "df_train_nr = df_train[df_train['category']!='Religion']\r\n",
    "df_test_r = df_test[df_test['category']=='Religion']\r\n",
    "df_test_nr = df_test[df_test['category']!='Religion']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(df_train_r), len(df_train_nr), len(df_test_r), len(df_test_nr))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import itertools\r\n",
    "def get_all_combinations(l : list) -> list:\r\n",
    "    ll = []\r\n",
    "    for L in range(0, len(l)+1):\r\n",
    "        for subset in itertools.combinations(l, L):\r\n",
    "            ll.append(list(subset))\r\n",
    "    return ll\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "column_names = [\"Religion\", \"lex_list\", \"ling_list\", \"user_list\", \"scores\", \"mean_score\"]\r\n",
    "df_record = pd.DataFrame(columns = column_names)\r\n",
    "\r\n",
    "# triversal all possible\r\n",
    "for lex_list in get_all_combinations([\"CL\", \"NVL\"]):\r\n",
    "    for ling_list in get_all_combinations([\"Length\",\"Modals\",\"Questions\", \"Links\"]):\r\n",
    "        for user_list in get_all_combinations([\"Gender\", \"RI\"]):\r\n",
    "            religion = True\r\n",
    "\r\n",
    "            for df_train, df_test in [[df_train_r, df_test_r], [df_train_nr, df_test_nr]]:\r\n",
    "\r\n",
    "                if religion:\r\n",
    "                    print(\"\\n==================Religion==================\")\r\n",
    "                else:\r\n",
    "                    print(\"\\n==================Non-Religion==================\")\r\n",
    "\r\n",
    "                x_train, x_test = get_features(df_train, df_test, model = args.model,lex_list=lex_list, ling_list=ling_list, user_list=user_list)\r\n",
    "\r\n",
    "\r\n",
    "                y_train, y_test = get_lable(df_train, df_test)\r\n",
    "                print('total features:', x_train.shape[1])\r\n",
    "\r\n",
    "                x = vstack([x_train,x_test])\r\n",
    "                y = y_train + y_test\r\n",
    "                clf = LogisticRegression(solver='liblinear')\r\n",
    "                start = time.time()\r\n",
    "                scores = cross_val_score(clf, x, y, cv=5 ,scoring='accuracy')\r\n",
    "                end = time.time()\r\n",
    "                print(\"Training Model Cost:\", round(end - start),'s.')\r\n",
    "                print(scores)\r\n",
    "                mean_score = np.mean(scores)\r\n",
    "                print(\"CV mean:\", mean_score)\r\n",
    "\r\n",
    "                record = {\"Religion\":religion, \"lex_list\":lex_list, \"ling_list\":ling_list, \"user_list\":user_list, \"scores\":scores, \"mean_score\":mean_score}\r\n",
    "                df_record = df_record.append(record,ignore_index=True)\r\n",
    "\r\n",
    "                religion = False\r\n",
    "\r\n",
    "df_record.to_csv(os.path.join('df_record.csv'))\r\n",
    "print(\"Wrote record to df_record.csv.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('NLP-hw1': conda)"
  },
  "interpreter": {
   "hash": "71e2e5b76bcbb14e570851f735b138803c85a1bc8fc18edcb67e91ecf28dd39d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}